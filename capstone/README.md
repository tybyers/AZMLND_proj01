# Using AzureML to Predict Malware Infections

This repo is my project submission for the Capstone of the [Udacity Azure Machine Learning Engineer Nanodegree](https://www.udacity.com/course/machine-learning-engineer-for-microsoft-azure-nanodegree--nd00333).

## Project Overview

For this project, we used a data set from the 2019 [Microsoft Malware Prediction](https://www.kaggle.com/c/microsoft-malware-prediction) Kaggle competition. Due to the large size of the data set, we used a truncated version of the data (only about 0.1% of the original data) so that we could complete all the tasks in a reasonable amount of time. More information about the data set is below in the *Dataset* section. 

We opted to treat this as a *classification* task, using *accuracy* as our metric.

For this project, we ran essentially two processes:

1. First we set up an Azure [AutoML](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml) experiment using the Python SDK. With this AutoML run, we attempted to find the best model -- that is, the model with the highest accuracy in predicting malware infections on our data set (using cross-validation).    
2. Next we ran an Azure [HyperDrive](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters) experiment, using XGBoost as our algorithm of choice, to tune three different hyperparameters and find the best-performing model (again using cross-validation).  

After determining our best model (spoiler alert: the AutoML VotingEnsemble model), we deployed it as an webservice/endpoint, tested consumption of the model using addtional data, and finally shut down the webservice.


## Project Set Up and Installation

In order to run this project yourself, you will need to:

  * Sign into [Microsoft Azure Machine Learning](https://ml.azure.com/).  
  * Set up a Compute Instance. *Note*: the Udacity Labs automatically set this up for us, with the following VM size: `STANDARD_DS3_V2 (4 Cores, 14 GB RAM, 28 GB Disk)`, and `CPU`.  
  * From the "Notebooks" tab, upload the following files from this repo:  
    * automl.ipynb  
    * automl_scoring.py  
    * hyperparameter_tuning.ipynb  
    * xgbtrain.py  
  * Make sure you have access to the data we have truncated and provided at: 
    * train data:  https://raw.githubusercontent.com/tybyers/AZMLND_projects/capstone/capstone/data/train_1_10k.csv  
    * test data: https://raw.githubusercontent.com/tybyers/AZMLND_projects/capstone/capstone/data/test_data.json 

## Dataset

### Overview

This data set was taken from the [Microsoft Malware Prediction](https://www.kaggle.com/c/microsoft-malware-prediction) competition run in 2019. "Microsoft is challenging the data science community to develop techniques to predict if a machine will soon be hit with malware. As with their previous, Malware Challenge (2015), Microsoft is providing Kagglers with an unprecedented malware dataset to encourage open-source progress on effective techniques for predicting malware occurrences."  

The raw training data set for the competition is very large: 8,921,483 observations (rows), and 83 features (variables), taking up 4.1 GB of space! 

In order to allow us to complete the tasks for this assignment in a reasonable amount of time, we took a very small subset of the entire train.csv data set -- just 10,000 rows (or about 0.1% of the entire training set). We skimmed off the top 10,000 rows from the data set for use in this project, and put it on our GitHub at https://raw.githubusercontent.com/tybyers/AZMLND_projects/capstone/capstone/data/train_1_10k.csv.  

### Task

The goal for this project is to predict whether a Windows machine is infected by various families of malware, based on different properties of that machine. Each row in this dataset corresponds to a machine, and has observations from telemetry data generated by WindowsDefender. The `HasDetections` column indicates that Malware was detected on the machine. 

For our 10,000 row data set, 4,950 machines have no malware detections, and 5,050 machines have malware detections. This is a nice, balanced data set, and basically the "baseline" accuracy for malware detection is 50.5% (that is, if our Machine Learner gets better than 50.5% accuracy, then it's better accuracy than if we called everything "infected").

### Access

I have uploaded the modified/truncated data to this GitHub repo and am accessing it in the following manner, using the Azure Python SDK:


```python
# Load Training Data set
data_path = 'https://raw.githubusercontent.com/tybyers/AZMLND_projects/capstone/capstone/data/train_1_10k.csv'
dataset = tdf.from_delimited_files(path=data_path)
dataset.to_pandas_dataframe().head()
```

```python
# Load Test Data Set
import requests
test_data_path = 'https://raw.githubusercontent.com/tybyers/AZMLND_projects/capstone/capstone/data/test_data.json'

r = requests.get(test_data_path)
input_data = r.text
```

## Automated ML

The first experiment was an AutoML experiment. Our settings and configuration for the experiment looked as thus:

```python
automl_settings = {
    "experiment_timeout_minutes": 30,
    "max_concurrent_iterations": 5,
    "primary_metric" : 'accuracy'
}

# TODO: Put your automl config here
automl_config = AutoMLConfig(
    compute_target=cpu_cluster,
    task="classification",
    training_data=dataset,
    label_column_name='HasDetections',
    enable_early_stopping=True,
    path=project_folder,
    debug_log='automl_errors.log',
    n_cross_validations=5,
    **automl_settings
)
```

Some of the config and settings were straightforward, but we chose other configurations for the following reasons:

  * `experiment_timeout_minutes = 30` -- Opted for 30 minutes so our Udacity-provided labs wouldn't time out, and this seemed to be sufficient to get through the training. Based on the RunDetails widget it appears a couple jobs were canceled, but since the overall results were pretty stable we judge this wasn't a large factor.
  * `max_concurrent_iterations = 5` -- Runs some iterations in parallel to speed up processing.  
  * `primary_metric = 'accuracy'` -- We opted for accuracy here, although AUC would have been a good choice too. Mostly opted for this for simplicity to compare it to the Hyperdrive run.  
  * `task = 'classification'` -- Since we're classifying whether a machine is infected or not, this is a natural choice.  
  * `label_column_name = 'HasDetections'` - This is the "target" column.  
  * `enable_early_stopping = True` - No need to keep going if models are doing well enough.  
  * `n_cross_validations = 5` - Wanted to do some cross validation. Especially important with a small data set with high cardinality.  

### Results

The best model overall was the `VotingEnsemble` model, with an accuracy of 0.6378. The remainder of the models were between about .505 and 0.624 (`StackedEnsemble` was 0.6357).

Here is a screeenshot of the `RunDetails` widget from the Jupyter notebook:

![automl_rundetails](./figs/automl_run_details.png).

If we look at the details of this model in the Azure ML Portal, we can see the AUC was 0.687 (which was also close to the winning AUC for the Kaggle competition using the larger version of this data set)

![automl_model_best](./figs/automl_model_best.png)

Next, we look the best model's Feature Importances, and we see only 5 features actually contributed meaningfully to the model. 

![best model feature importance](./figs/automl_feature_importance.png) 

### Registering the Best Model

We then registered the best model. Below is a screenshot of the Azure ML Portal with the best model saved (note we also have the best HyperDrive model too). 

![autmol_best_model_model_list](./figs/automl_best_model_model_list.png)

Note the Run ID from the highlighted line above, and compare to the Run ID from the Run details list in the previous section. 

Next, we compared this model to the HyperDrive-tuned model to see which one we should deploy. 

## Hyperparameter Tuning

For the Hyperparameter tuning experiment, done via HyperDrive, we opted for tuning an XGBoost model. We did this because, of the top 10 performing AutoML models, 4 of them were XGBoost Classifiers. We are unsure how much hyperparameter tuning, if any, happens with AutoML, so we wanted to see if we could do any better than the AutoML XGBoost.

### Details  

For the HyperDrive experiment, we had to write an entry script, which can be see in the "xgbtrain.py" file in this repo.  This script allows up to 5 parameters to be passed into it -- only 3 of which we actually used for tuning the hyperparameters.  

We opted to tune the following hyperparameters because, based on our experience using XGBoost in the past, these are some of the highest-leverage hyperparameters to tune:

```python
param_sampling = RandomParameterSampling({'--max_depth': choice(range(2,11)),
                                         '--n_estimators': choice(25, 50, 100, 250, 500, 750, 1000),
                                         '--learning_rate': uniform(0, 1.0)})`
```
* `max_depth` - Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
* `n_estimators` - Number of boosting rounds. Trains faster when smaller; too large can overfit.
* `learning_rate` - Step size shrinkage used in update to prevents overfitting. Range: [0, 1]

The remainder of our HyperDrive configuration is below, with an explanation following. 

```python
early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)

src = ScriptRunConfig(source_directory=project_folder,
                      script='xgbtrain.py',
#                      arguments=['--kernel', 'linear', '--penalty', 1.0],
                      compute_target=cpu_cluster,
                      environment=hyperdrive_env)

hyperdrive_config = HyperDriveConfig(run_config=src,
                                    hyperparameter_sampling=param_sampling,
                                    policy=early_termination_policy,
                                    primary_metric_name='Accuracy',
                                    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,
                                    max_total_runs=100,
                                    max_concurrent_runs=4)
```

* `early_termination_policy = BanditPolicy` - This is an early termination policy that terminates any runs where the metric is not performing well compared to the best-performing run. The benefit of this policy is that especially for long-running trials it will terminate early if the accuracy metric is too far off the best run so far, and in that way it will speed up the overall compute.  
* `environment=hyperdrive_env` -- Explained below  
* `primary_metric_name='Accuracy'` -- Opted for Accuracy here to be consistent with the AutoML experiment.  
* `primary_metric_goal=PrimaryMetricGoal.MAXIMIZE` -- Obviously we want the highest accuracy and not the lowest :). 
* `max_total_runs=100` -- XGBoost can train fairly fast, so let's see if we get any real outstanding hits. Even so, limit to only 100 runs.  
* `max_concurrent_runs=4` -- Run up to 4 models in parallel.  

### Environment

We also had to create a custom conda environment to run the experiment. 

We did so using some Jupyter cell magic, which wrote a new file called "hyperdrive_dependencies.yml":

```python
%%writefile hyperdrive_dependencies.yml

dependencies:
- python=3.6.2
- scikit-learn
- pandas 
- numpy
- pip:
    - azureml-defaults
    - xgboost
```

Then we created a conda environment from this file:

```python
hyperdrive_env = Environment.from_conda_specification(name = 'hyperdrive-env', file_path = './hyperdrive_dependencies.yml')
```

### Results

Next, we ran the experiment! Note that this was a 5-fold cross-validation, just like for AutoML. 

The best model gave an accuracy of 0.626, and the worst model and accuracy of 0.5785.

The best model's parameters were: 
  * `max_depth`: 2  
  * `n_estimators`: 100  
  * `learning_rate`: ~0.19  

The worst model's parameters were:  
  * `max_depth`: 9  
  * `n_estimators`: 25  
  * `learning_rate`: ~0.93

In general, we noticed that a small `max_depth` (2 or 3), at least 100 estimators, and a low `learning_rate` helped achieve a better score. This is probably due to the high cardinality of this data set with a relatively small number of observations, so it would be easy to overfit.    

Below is are two screenshots of the RunDetails widget. As we can see, there wasn't a huge amount of variability in the results between the various models.   

![hyperdrive run details](./figs/hd_rundetails.png)

![hyperdrive run details2](./figs/hd_rundetails2.png)

One fun note -- our best HyperDrive XGBoost model performed better than the best XGBoost AutoML model by an Accuracy of 0.002! (still worse than the VotingEnsemble, but better than the best XGBoost model!)

Looking in the Azure ML Portal, we can confirm what we are seeing within the Jupyter notebooks, regarding the best model and its hyperparameters: 

![hd_model_list](./figs/hd_parameter_samp.png)

And finally another look at the details of the top model and the hyperparameters. 
![hd_portal](./figs/hd_portal_topmodel_details.png) 

### Registering the Best Model

Just like for the automl model, we also registered the best hyperdrive model. Below is a screenshot of the Azure ML Portal with the best model saved. 

![hyperdrive_model_list](./figs/hd_model_list.png)

Note the Run ID from the highlighted line above, and compare to the Run ID from the Run details list in the previous section. 

Next, we compared this model to the HyperDrive-tuned model to see which one we should deploy. 

## Model Deployment

Finally, we deployed the best model -- which happened to be the `VotingEnsemble` model from the AutoML run, as an endpoint.  

The details for doing this are in the `automl.ipynb` Jupyter notebook (screenshot below), starting at the "Model Deployment" section. After registering our best model, we then created an Inference and Deployment config, and then deployed as a Web Service.

![automl deploy](./figs/automl_deploy_jupyter.png)

We can see below the endpoint as active in the Azure ML Portal:

![automl_as_endpoint](./figs/automl_as_endpoint.png)

Clicking into the endpoint we see the details of the endpoint:

![automl endpoint details](./figs/automl_endpoint_details.png)

and information related to how to consume the endpoint:  

![automl endpoint consume](./figs/automl_endpoint_consume.png)  

### Testing Endpoint

We then tested the endpoint with 5 sample data points that we harvested from the original Kaggle data set (not part of the 10,000 rows we grabbed used to train the model).

![test_endpoint.png](./figs/test_endpoint.png) 

The URL for that test data set is at: 'https://raw.githubusercontent.com/tybyers/AZMLND_projects/capstone/capstone/data/test_data.json' 

The "automl_scoring.py" file in this repo is the file that we use to format the test data and send it to the web service.

Our quick little test of 5 data points got 3/5 correct -- that is, 60% accuracy, or well in line with our expectations from our HyperDrive run.  


## Screen Recording
*TODO* Provide a link to a screen recording of the project in action. Remember that the screencast should demonstrate:
- A working model
- Demo of the deployed  model
- Demo of a sample request sent to the endpoint and its response

